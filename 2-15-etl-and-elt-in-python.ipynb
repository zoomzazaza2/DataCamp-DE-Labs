{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a9fdd6",
   "metadata": {},
   "source": [
    "Running an ETL Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e7c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the raw_data.csv file\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Transform the extracted_data\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load the transformed_data to cleaned_data.csv\n",
    "load(data_frame=transformed_data, target_table=\"cleaned_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25c50b",
   "metadata": {},
   "source": [
    "ELT in Action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde96137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the raw_data.csv file\n",
    "raw_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Load the extracted_data to the raw_data table\n",
    "load(data_frame=raw_data, table_name=\"raw_data\")\n",
    "\n",
    "# Transform data in the raw_data table\n",
    "transform(\n",
    "  source_table=\"raw_data\", \n",
    "  target_table=\"cleaned_data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db63f4",
   "metadata": {},
   "source": [
    "Building an ETL Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data_frame, file_name):\n",
    "  # Write cleaned_data to a CSV using file_name\n",
    "  data_frame.to_csv(file_name)\n",
    "  print(f\"Successfully loaded data to {file_name}\")\n",
    "\n",
    "extracted_data = extract(file_name=\"raw_data.csv\")\n",
    "\n",
    "# Transform extracted_data using transform() function\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Load transformed_data to the file transformed_data.csv\n",
    "load(data_frame=transformed_data, file_name=\"transformed_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2bd1a",
   "metadata": {},
   "source": [
    "The \"T\" in ELT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e2726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete building the transform() function\n",
    "def transform(source_table, target_table):\n",
    "  data_warehouse.execute(f\"\"\"\n",
    "  CREATE TABLE {target_table} AS\n",
    "      SELECT\n",
    "          CONCAT(\"Product ID: \", product_id),\n",
    "          quantity * price\n",
    "      FROM {source_table};\n",
    "  \"\"\")\n",
    "\n",
    "extracted_data = extract(file_name=\"raw_sales_data.csv\")\n",
    "load(data_frame=extracted_data, table_name=\"raw_sales_data\")\n",
    "\n",
    "# Populate total_sales by transforming raw_sales_data\n",
    "transform(source_table=\"raw_sales_data\", target_table=\"total_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a399df",
   "metadata": {},
   "source": [
    "Extracting, Transforming, and Loading Student Scores Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_name):\n",
    "  return pd.read_csv(file_name)\n",
    "\n",
    "def transform(data_frame):\n",
    "  return data_frame.loc[:, [\"industry_name\", \"number_of_firms\"]]\n",
    "\n",
    "def load(data_frame, file_name):\n",
    "  data_frame.to_csv(file_name)\n",
    "  \n",
    "extracted_data = extract(file_name=\"raw_industry_data.csv\")\n",
    "transformed_data = transform(data_frame=extracted_data)\n",
    "\n",
    "# Pass the transformed_data DataFrame to the load() function\n",
    "load(data_frame=transformed_data, file_name=\"number_of_firms.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39544a98",
   "metadata": {},
   "source": [
    "Extracting data from parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a53144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the sales data into a DataFrame\n",
    "sales_data = pd.read_parquet(\"sales_data.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "# Check the data type of the columns of the DataFrames\n",
    "print(sales_data.dtypes)\n",
    "\n",
    "# Print the shape of the DataFrame, as well as the head\n",
    "print(sales_data.shape)\n",
    "print(sales_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b92d23",
   "metadata": {},
   "source": [
    "Pulling data from SQL databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "\n",
    "# Create a connection to the sales database\n",
    "db_engine = sqlalchemy.create_engine(\"postgresql+psycopg2://repl:password@localhost:5432/sales\")\n",
    "\n",
    "# Query the sales table\n",
    "raw_sales_data = pd.read_sql(\"SELECT * FROM sales\", db_engine)\n",
    "print(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b62f5a",
   "metadata": {},
   "source": [
    "Building functions to extract data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1640c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    connection_uri = \"postgresql+psycopg2://repl:password@localhost:5432/sales\"\n",
    "    db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "    raw_data = pd.read_sql(\"SELECT * FROM sales WHERE quantity_ordered = 1\", db_engine)\n",
    "    \n",
    "    # Print the head of the DataFrame\n",
    "    print(raw_data.head())\n",
    "    \n",
    "    # Return the extracted DataFrame\n",
    "    return raw_data\n",
    "    \n",
    "# Call the extract() function\n",
    "raw_sales_data = extract()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78336b86",
   "metadata": {},
   "source": [
    "Filtering pandas DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the sales_data.parquet path\n",
    "raw_sales_data = extract(\"sales_data.parquet\")\n",
    "\n",
    "def transform(raw_data):\n",
    "  \t# Only keep rows with `Quantity Ordered` greater than 1\n",
    "    clean_data = raw_data.loc[raw_data['Quantity Ordered']>1,:]\n",
    "    \n",
    "    # Only keep columns \"Order Date\", \"Quantity Ordered\", and \"Purchase Address\"\n",
    "    clean_data = clean_data.loc[:,[\"Order Date\", \"Quantity Ordered\", \"Purchase Address\"]]\n",
    "    \n",
    "    # Return the filtered DataFrame\n",
    "    return clean_data\n",
    "    \n",
    "transform(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae9f25",
   "metadata": {},
   "source": [
    "Transforming sales data with pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ebaf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sales_data = extract(\"sales_data.csv\")\n",
    "\n",
    "def transform(raw_data):\n",
    "    # Convert the \"Order Date\" column to type datetime\n",
    "    raw_data[\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n",
    "    \n",
    "    # Only keep items under ten dollars\n",
    "    clean_data = raw_data.loc[raw_data['Price Each']<10, :]\n",
    "    return clean_data\n",
    "\n",
    "clean_sales_data = transform(raw_sales_data)\n",
    "\n",
    "# Check the data types of each column\n",
    "print(clean_sales_data.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a485bba",
   "metadata": {},
   "source": [
    "Validating data transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    raw_data = pd.read_parquet(file_path)\n",
    "    return raw_data\n",
    "\n",
    "raw_sales_data = extract(\"sales_data.parquet\")\n",
    "\n",
    "def transform(raw_data):\n",
    "  \t# Filter rows and columns\n",
    "    clean_data = raw_data.loc[raw_data['Quantity Ordered']==1, [\"Order ID\", \"Price Each\" , \"Quantity Ordered\"]]\n",
    "    return clean_data\n",
    "\n",
    "# Transform the raw_sales_data\n",
    "clean_sales_data = transform(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ad0425",
   "metadata": {},
   "source": [
    "Loading sales data to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\t# Find the items prices less than 25 dollars\n",
    "\treturn raw_data.loc[raw_data[\"Price Each\"] < 25, [\"Order ID\", \"Product\", \"Price Each\", \"Order Date\"]]\n",
    "\n",
    "def load(clean_data):\n",
    "\t# Write the data to a CSV file without the index column\n",
    "\tclean_data.to_csv(\"transformed_sales_data.csv\", index=False)\n",
    "\n",
    "\n",
    "clean_sales_data = transform(raw_sales_data)\n",
    "\n",
    "# Call the load function on the cleaned DataFrame\n",
    "load(clean_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a8fb4",
   "metadata": {},
   "source": [
    "Customizing a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the os library\n",
    "import os\n",
    "\n",
    "# Load the data to a csv file with the index, no header and pipe separated\n",
    "def load(clean_data, path_to_write):\n",
    "\tclean_data.to_csv(path_to_write, header=False, sep=\"|\")\n",
    "\n",
    "load(clean_sales_data, \"clean_sales_data.csv\")\n",
    "\n",
    "# Check that the file is present.\n",
    "file_exists = os.path.exists('clean_sales_data.csv')\n",
    "print(file_exists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d34de",
   "metadata": {},
   "source": [
    "Persisting data to files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(clean_data, file_path):\n",
    "    # Write the data to a file\n",
    "    clean_data.to_csv(file_path, header=False, index=False)\n",
    "\n",
    "    # Check to make sure the file exists\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    if not file_exists:\n",
    "        raise Exception(f\"File does NOT exists at path {file_path}\")\n",
    "\n",
    "# Load the transformed data to the provided file path\n",
    "load(clean_sales_data, \"transformed_sales_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe8af4",
   "metadata": {},
   "source": [
    "Logging within a data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb262062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "    raw_data[\"Order Date\"] = pd.to_datetime(raw_data[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n",
    "    clean_data = raw_data.loc[raw_data[\"Price Each\"] < 10, :]\n",
    "    \n",
    "    # Create an info log regarding transformation\n",
    "    logging.info(\"Transformed 'Order Date' column to type 'datetime'.\")\n",
    "    \n",
    "    # Create debug-level logs for the DataFrame before and after filtering\n",
    "    logging.debug(f\"Shape of the DataFrame before filtering: {raw_data.shape}\")\n",
    "    logging.debug(f\"Shape of the DataFrame after filtering: {clean_data.shape}\")\n",
    "    \n",
    "    return clean_data\n",
    "  \n",
    "clean_sales_data = transform(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7448f",
   "metadata": {},
   "source": [
    "Handling exceptions when loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "    return pd.read_parquet(file_path)\n",
    "\n",
    "# Update the pipeline to include a try block\n",
    "try:\n",
    "\t# Attempt to read in the file\n",
    "    raw_sales_data = extract(\"sales_data.parquet\")\n",
    "\t\n",
    "# Catch the FileNotFoundError\n",
    "except FileNotFoundError as file_not_found:\n",
    "\t# Write an error-level log\n",
    "\tlogging.error(file_not_found)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e2855",
   "metadata": {},
   "source": [
    "Monitoring and alerting within a data pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d83b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\treturn raw_data.loc[raw_data[\"Total Price\"] > 1000, :]\n",
    "\n",
    "try:\n",
    "\tclean_sales_data = transform(raw_sales_data)\n",
    "\tlogging.info(\"Successfully filtered DataFrame by 'Total Price'\")\n",
    "\n",
    "except KeyError as ke:\n",
    "\tlogging.warning(f\"{ke}: Cannot filter DataFrame by 'Total Price'\")\n",
    "\t\n",
    "\t# Create the \"Total Price\" column, transform the updated DataFrame\n",
    "\traw_sales_data[\"Total Price\"] = raw_sales_data[\"Price Each\"] * raw_sales_data[\"Quantity Ordered\"]\n",
    "\tclean_sales_data = transform(raw_sales_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881791e7",
   "metadata": {},
   "source": [
    "Ingesting JSON data with pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e44f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "  # Read the JSON file into a DataFrame\n",
    "  return pd.read_json(file_path, orient=\"records\")\n",
    "\n",
    "# Call the extract function with the appropriate path, assign to raw_testing_scores\n",
    "raw_testing_scores = extract('testing_scores.json')\n",
    "\n",
    "# Output the head of the DataFrame\n",
    "print(raw_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f74835",
   "metadata": {},
   "source": [
    "Reading JSON data into memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa71d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(file_path):\n",
    "  \t# Read the JSON file into a DataFrame, orient by index\n",
    "\treturn pd.read_json(file_path, orient=\"index\")\n",
    "\n",
    "# Call the extract function, pass in the desired file_path\n",
    "raw_testing_scores = extract(\"nested_scores.json\")\n",
    "print(raw_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb613d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the json library\n",
    "import json\n",
    "\n",
    "def extract(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        # Load the data from the JSON file\n",
    "        raw_data = json.load(json_file)\n",
    "    return raw_data\n",
    "\n",
    "raw_testing_scores = extract(\"nested_scores.json\")\n",
    "\n",
    "# Print the raw_testing_scores\n",
    "print(raw_testing_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e17abd",
   "metadata": {},
   "source": [
    "Iterating over dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_testing_scores_keys = []\n",
    "\n",
    "# Iterate through the keys of the raw_testing_scores dictionary\n",
    "for school_id in raw_testing_scores.keys():\n",
    "  \t# Append each key to the raw_testing_scores_keys list\n",
    "\traw_testing_scores_keys.append(school_id)\n",
    "    \n",
    "print(raw_testing_scores_keys[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c59285",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_testing_scores_values = []\n",
    "\n",
    "# Iterate through the values of the raw_testing_scores dictionary\n",
    "for school_info in raw_testing_scores.values():\n",
    "\traw_testing_scores_values.append(school_info)\n",
    "    \n",
    "print(raw_testing_scores_values[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_testing_scores_keys = []\n",
    "raw_testing_scores_values = []\n",
    "\n",
    "# Iterate through the values of the raw_testing_scores dictionary\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\traw_testing_scores_keys.append(school_id)\n",
    "\traw_testing_scores_values.append(school_info)\n",
    "\n",
    "print(raw_testing_scores_keys[0:3])\n",
    "print(raw_testing_scores_values[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66223f",
   "metadata": {},
   "source": [
    "Parsing data from dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the street_address from the dictionary\n",
    "street_address = school.get(\"street_address\")\n",
    "\n",
    "# Parse the scores dictionary\n",
    "scores = school.get(\"scores\")\n",
    "\n",
    "# Try to parse the math, reading and writing values from scores\n",
    "math_score = scores.get(\"math\", 0)\n",
    "reading_score = scores.get(\"reading\", 0)\n",
    "writing_score = scores.get(\"writing\", 0)\n",
    "\n",
    "print(f\"Street Address: {street_address}\")\n",
    "print(f\"Math: {math_score}, Reading: {reading_score}, Writing: {writing_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f38bc7",
   "metadata": {},
   "source": [
    "Transforming JSON data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dfc7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_testing_scores = []\n",
    "\n",
    "# Loop through each of the dictionary key-value pairs\n",
    "for school_id, school_info in raw_testing_scores.items():\n",
    "\tnormalized_testing_scores.append([\n",
    "    \tschool_id,\n",
    "    \tschool_info.get(\"street_address\"),  # Pull the \"street_address\"\n",
    "    \tschool_info.get(\"city\"),\n",
    "    \tschool_info.get(\"scores\").get(\"math\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"reading\", 0),\n",
    "    \tschool_info.get(\"scores\").get(\"writing\", 0),\n",
    "    ])\n",
    "\n",
    "print(normalized_testing_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec991659",
   "metadata": {},
   "source": [
    "Transforming and cleaning DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81957e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the normalized_testing_scores list\n",
    "normalized_data = pd.DataFrame(normalized_testing_scores)\n",
    "\n",
    "# Set the column names\n",
    "normalized_data.columns = [\"school_id\", \"street_address\", \"city\", \"avg_score_math\", \"avg_score_reading\", \"avg_score_writing\"]\n",
    "\n",
    "normalized_data = normalized_data.set_index(\"school_id\")\n",
    "print(normalized_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effba5c9",
   "metadata": {},
   "source": [
    "Filling missing values with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of the `raw_testing_scores` DataFrame\n",
    "print(raw_testing_scores.head())\n",
    "\n",
    "# Fill NaN values with the average from that column\n",
    "raw_testing_scores[\"math_score\"] = raw_testing_scores[\"math_score\"].fillna(raw_testing_scores[\"math_score\"].mean())\n",
    "\n",
    "# Print the head of the raw_testing_scores DataFrame\n",
    "print(raw_testing_scores.head())\n",
    "\n",
    "def transform(raw_data):\n",
    "\traw_data.fillna(\n",
    "    \tvalue={\n",
    "\t\t\t# Fill NaN values with column mean\n",
    "\t\t\t\"math_score\": raw_data[\"math_score\"].mean(),\n",
    "\t\t\t\"reading_score\": raw_data[\"reading_score\"].mean(),\n",
    "\t\t\t\"writing_score\": raw_data[\"writing_score\"].mean()\n",
    "\t\t}, inplace=True\n",
    "\t)\n",
    "\treturn raw_data\n",
    "\n",
    "clean_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "# Print the head of the clean_testing_scores DataFrame\n",
    "print(clean_testing_scores.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316bf62",
   "metadata": {},
   "source": [
    "Grouping data with pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\t# Use .loc[] to only return the needed columns\n",
    "\traw_data = raw_data.loc[:,[\"city\", \"math_score\", \"reading_score\", \"writing_score\" ]]\n",
    "\t\n",
    "    # Group the data by city, return the grouped DataFrame\n",
    "\tgrouped_data = raw_data.groupby(by=[\"city\"], axis=0).mean()\n",
    "\treturn grouped_data\n",
    "\n",
    "# Transform the data, print the head of the DataFrame\n",
    "grouped_testing_scores = transform(raw_testing_scores)\n",
    "print(grouped_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425f751",
   "metadata": {},
   "source": [
    "Applying advanced transformations to DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bdce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(raw_data):\n",
    "\t# Use the apply function to extract the street_name from the street_address\n",
    "    raw_data[\"street_name\"] = raw_data.apply(\n",
    "   \t\t# Pass the correct function to the apply method\n",
    "        find_street_name, \n",
    "        ######## No need for parenthesies ( ) after function ########\n",
    "        axis=1\n",
    "    )\n",
    "    return raw_data\n",
    "\n",
    "# Transform the raw_testing_scores DataFrame\n",
    "cleaned_testing_scores = transform(raw_testing_scores)\n",
    "\n",
    "# Print the head of the cleaned_testing_scores DataFrame\n",
    "print(cleaned_testing_scores.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e91bc",
   "metadata": {},
   "source": [
    "Loading data to a Postgres database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the connection string, create the connection object to the schools database\n",
    "db_engine = sqlalchemy.create_engine(\"postgresql+psycopg2://repl:password@localhost:5432/schools\")\n",
    "\n",
    "# Write the DataFrame to the scores table\n",
    "cleaned_testing_scores.to_sql(\n",
    "\tname=\"scores\",\n",
    "\tcon=db_engine,\n",
    "\tindex=False,\n",
    "\tif_exists=\"replace\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fee8aa",
   "metadata": {},
   "source": [
    "Validating data loaded to a Postgres Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c2ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(clean_data, con_engine):\n",
    "\t# Store the data in the schools database\n",
    "    clean_data.to_sql(\n",
    "    \tname=\"scores_by_city\",\n",
    "\t\tcon=con_engine,\n",
    "\t\tif_exists=\"replace\",  # Make sure to replace existing data\n",
    "\t\tindex=True,\n",
    "\t\tindex_label=\"school_id\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(clean_data, con_engine):\n",
    "    clean_data.to_sql(name=\"scores_by_city\", con=con_engine, if_exists=\"replace\", index=True, index_label=\"school_id\")\n",
    "    \n",
    "# Call the load function, passing in the cleaned DataFrame\n",
    "load(cleaned_testing_scores, db_engine)\n",
    "\n",
    "# Call query the data in the scores_by_city table, check the head of the DataFrame\n",
    "to_validate = pd.read_sql(\"SELECT * FROM scores_by_city\", con=db_engine)\n",
    "print(to_validate.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248fdc48",
   "metadata": {},
   "source": [
    "Validating a data pipeline at \"checkpoints\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "load(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\n",
    "print(f\"Shape of raw_tax_data: {raw_tax_data.shape}\")\n",
    "print(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "\n",
    "to_validate = pd.read_parquet(\"clean_tax_data.parquet\")\n",
    "print(clean_tax_data.head(3))\n",
    "print(to_validate.head(3))\n",
    "\n",
    "# Check that the DataFrames are equal\n",
    "print(to_validate.equals(clean_tax_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d159766",
   "metadata": {},
   "source": [
    "Testing a data pipeline end-to-end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff21872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the data pipeline to run three times\n",
    "for attempt in range(0, 3):\n",
    "\tprint(f\"Attempt: {attempt}\")\n",
    "\traw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "\t\n",
    "\t# Print the shape of the cleaned_tax_data DataFrame\n",
    "\tprint(f\"Shape of clean_tax_data: {clean_tax_data.shape}\")\n",
    "    \n",
    "# Read in the loaded data, check the shape\n",
    "to_validate = pd.read_parquet(\"clean_tax_data.parquet\")\n",
    "print(f\"Final shape of cleaned data: {to_validate.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9cd2f",
   "metadata": {},
   "source": [
    "Validating a data pipeline with assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "\n",
    "# Validate the number of columns in the DataFrame\n",
    "assert len(clean_tax_data.columns) == 5\n",
    "\n",
    "# Determine if the clean_tax_data DataFrames take type pd.DataFrame\n",
    "assert isinstance(clean_tax_data, pd.DataFrame)\n",
    "\n",
    "# Assert that clean_tax_data is an instance of a pd.DataFrame\n",
    "assert isinstance(clean_tax_data, pd.DataFrame)\n",
    "\n",
    "\n",
    "# Assert that clean_tax_data takes is an instance of a string\n",
    "try:\n",
    "\tassert isinstance(clean_tax_data, str)\n",
    "except Exception as e:\n",
    "\tprint(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35e140",
   "metadata": {},
   "source": [
    "Writing unit tests with pytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6197764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# to test with pytest, run 'python3 -m pytest' in the terminal\n",
    "\n",
    "def test_transformed_data():\n",
    "    raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "    clean_tax_data = transform(raw_tax_data)\n",
    "    \n",
    "    # Assert that the transform function returns a pd.DataFrame\n",
    "    assert isinstance(clean_tax_data, pd.DataFrame)\n",
    "    \n",
    "    # Assert that the clean_tax_data DataFrame has more columns than the raw_tax_data DataFrame\n",
    "    assert len(clean_tax_data.columns) > len(raw_tax_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a00ea",
   "metadata": {},
   "source": [
    "Creating fixtures with pytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ccc2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pytest\n",
    "import pytest\n",
    "\n",
    "# Create a pytest fixture\n",
    "@pytest.fixture()\n",
    "def raw_tax_data():\n",
    "\traw_data = extract(\"raw_tax_data.csv\")\n",
    "    \n",
    "    # Return the raw DataFrame\n",
    "\treturn raw_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467659b",
   "metadata": {},
   "source": [
    "Unit testing a data pipeline with fixtures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4042c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @pytest.fixture() will send the returned value to next test function.\n",
    "@pytest.fixture()\n",
    "def clean_tax_data():\n",
    "    raw_data = pd.read_csv(\"raw_tax_data.csv\")\n",
    "    clean_data = transform(raw_data)\n",
    "    return clean_data\n",
    "\n",
    "# Pass the fixture to the function\n",
    "def test_tax_rate(clean_tax_data):\n",
    "    # Assert values are within the expected range\n",
    "    assert clean_tax_data[\"tax_rate\"].max() <= 1 and clean_tax_data[\"tax_rate\"].min() >= 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef4a7e",
   "metadata": {},
   "source": [
    "Data pipeline architecture patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the extract, transform, and load functions from pipeline_utils\n",
    "from pipeline_utils import extract, transform, load\n",
    "# แยก definition of E/T/L ไว้อีกไฟลเพื่อความ clean\n",
    "\n",
    "# Run the pipeline end to end by extracting, transforming and loading the data\n",
    "raw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "clean_tax_data = transform(raw_tax_data)\n",
    "load(clean_tax_data, \"clean_tax_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c3cd0",
   "metadata": {},
   "source": [
    "Running a data pipeline end-to-end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777cbecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pipeline_utils import extract, transform, load\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "# กำหนดค่าพื้นฐาน (basic configuration) สำหรับระบบ logging\n",
    "\n",
    "# format= \n",
    "# %(levelname)s → ชื่อระดับของ log เช่น DEBUG, INFO, WARNING, ERROR\n",
    "# %(message)s → ข้อความจริง ๆ ที่เราพิมพ์ตอนเรียก log\n",
    "# Ex. 'DEBUG: This is a debug message'\n",
    "\n",
    "\n",
    "# level= ระดับขั้นต่ำของ log ที่จะถูกแสดง โดยเริ่มจาก\n",
    "# DEBUG < INFO < WARNING < ERROR < CRITICAL\n",
    "# level=logging.DEBUG จึงหมายถึงทำการทำ log ทุกระดับ\n",
    "\n",
    "try:\n",
    "\traw_tax_data = extract(\"raw_tax_data.csv\")\n",
    "\tclean_tax_data = transform(raw_tax_data)\n",
    "\tload(clean_tax_data, \"clean_tax_data.parquet\")\n",
    "    \n",
    "\tlogging.info(\"Successfully extracted, transformed and loaded data.\")  # Log a success message.\n",
    "    \n",
    "except Exception as e:\n",
    "\tlogging.error(f\"Pipeline failed with error: {e}\")  # Log failure message\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
