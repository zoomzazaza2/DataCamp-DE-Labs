{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb47859a",
   "metadata": {},
   "source": [
    "Creating a SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470489b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test_spark\").getOrCreate()\n",
    "print(\"Spark version:\", spark.version)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddcba56",
   "metadata": {},
   "source": [
    "Loading census data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0eb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CSV\n",
    "census_adult = spark.read.csv(\"adult_reduced.csv\")\n",
    "\n",
    "# Show the DataFrame\n",
    "census_adult.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6a479",
   "metadata": {},
   "source": [
    "Reading a CSV and performing aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "salaries_df = spark.read.csv(\"salaries.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Count the total number of rows\n",
    "row_count = salaries_df.count()\n",
    "print(f\"Total rows: {row_count}\")\n",
    "\n",
    "# Group by company size and calculate the average of salaries\n",
    "salaries_df.groupBy(\"company_size\").agg({\"salary_in_usd\": \"avg\"}).show()\n",
    "salaries_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aece30",
   "metadata": {},
   "source": [
    "Filtering by company\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3062a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average salary for entry level in Canada\n",
    "CA_jobs = ca_salaries_df.filter(ca_salaries_df[\"company_location\"] == \"CA\").filter(ca_salaries_df['experience_level']\n",
    " == \"EN\").groupBy().avg(\"salary_in_usd\")\n",
    "\n",
    "# Show the result\n",
    "CA_jobs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb6933",
   "metadata": {},
   "source": [
    "Infer and filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee48919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "census_df = spark.read.json(\"adults.json\")\n",
    "\n",
    "# Filter rows based on age condition\n",
    "salary_filtered_census = census_df.where(census_df[\"age\"]>40)\n",
    "\n",
    "# Show the result\n",
    "salary_filtered_census.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6975d85",
   "metadata": {},
   "source": [
    "Schema writeout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfcc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Fill in the schema with the columns you need from the exercise instructions\n",
    "schema = StructType([StructField(\"age\",IntegerType()),\n",
    "                     StructField(\"education_num\",IntegerType()),\n",
    "                     StructField(\"marital_status\",StringType()),\n",
    "                     StructField(\"occupation\",StringType()),\n",
    "                     StructField(\"income\",StringType()),\n",
    "                    ])\n",
    "\n",
    "# Read in the CSV, using the schema you defined above\n",
    "census_adult = spark.read.csv(\"adult_reduced_100.csv\", sep=',', header=False, schema=schema)\n",
    "\n",
    "# Print out the schema\n",
    "census_adult.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6f457",
   "metadata": {},
   "source": [
    "Handling missing data with fill and drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b758545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any nulls\n",
    "census_cleaned = census_df.na.drop()\n",
    "\n",
    "# Show the result\n",
    "census_cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9218fe6",
   "metadata": {},
   "source": [
    "Column operations - creating and renaming columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'weekly_salary'\n",
    "census_df_weekly = census_df.withColumn(\"weekly_salary\", census_df[\"income\"]/52)\n",
    "\n",
    "# Rename the 'age' column to 'years'\n",
    "census_df_weekly = census_df_weekly.withColumnRenamed(\"age\", \"years\")\n",
    "\n",
    "# Show the result\n",
    "census_df_weekly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236ba84",
   "metadata": {},
   "source": [
    "Joining flights with their destination airports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913dc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data\n",
    "airports.show()\n",
    "\n",
    "# .withColumnRenamed() renames the \"faa\" column to \"dest\"\n",
    "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
    "\n",
    "# Join the DataFrames\n",
    "flights_with_airports = flights.join(airports,\"dest\", \"leftouter\")\n",
    "\n",
    "# Examine the new DataFrame\n",
    "#flights.show()\n",
    "flights_with_airports.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012cb123",
   "metadata": {},
   "source": [
    "Integers in PySpark UDFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd4574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the function age_category as a UDF\n",
    "age_category_udf = udf(age_category, StringType())\n",
    "\n",
    "# Apply your udf to the DataFrame\n",
    "age_category_df_2 = age_category_df.withColumn(\"category\", age_category_udf(age_category_df[\"age\"]))\n",
    "\n",
    "# Show df\n",
    "age_category_df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcce167",
   "metadata": {},
   "source": [
    "Pandas UDFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pandas UDF that adds 10 to each element in a vectorized way\n",
    "@pandas_udf(DoubleType())\n",
    "def add_ten_pandas(column):\n",
    "    return column + 10\n",
    "\n",
    "# Apply the UDF and show the result\n",
    "df.withColumn(\"10_plus\", add_ten_pandas(df['value']))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b5b2f",
   "metadata": {},
   "source": [
    "Creating RDDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb378546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = spark.read.csv(\"salaries.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert DataFrame to RDD\n",
    "rdd = df.rdd\n",
    "\n",
    "# Show the RDD's contents\n",
    "rdd.collect()\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54643d48",
   "metadata": {},
   "source": [
    "Collecting RDDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08387af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from the df_salaries\n",
    "rdd_salaries = df_salaries.rdd\n",
    "\n",
    "# Collect and print the results\n",
    "print(rdd_salaries.collect())\n",
    "\n",
    "# Group by the experience level and calculate the maximum salary\n",
    "dataframe_results = df_salaries.groupby(\"experience_level\").agg({\"salary_in_usd\": 'max'})\n",
    "\n",
    "# Show the results\n",
    "dataframe_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa5cb2",
   "metadata": {},
   "source": [
    "Querying on a temp view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as a view\n",
    "df.createOrReplaceTempView(\"data_view\")\n",
    "\n",
    "# Advanced SQL query: Calculate total salary by Position\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT Position, SUM(Salary) AS Total_Salary\n",
    "    FROM data_view\n",
    "    GROUP BY Position\n",
    "    ORDER BY Total_Salary DESC\n",
    "    \"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4e1f3",
   "metadata": {},
   "source": [
    "Running SQL on DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def62245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary table \"people\"\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Select the names from the temporary table people\n",
    "query = \"\"\"SELECT name FROM people\"\"\"\n",
    "\n",
    "# Assign the result of Spark's query to people_df_names\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "# Print the top 10 names of the people\n",
    "people_df_names.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaee5b0",
   "metadata": {},
   "source": [
    "Analytics with SQL on DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view of salaries_table\n",
    "salaries_df.createOrReplaceTempView('salaries_table')\n",
    "\n",
    "# Construct the \"query\"\n",
    "query = '''SELECT job_title, salary_in_usd FROM salaries_table WHERE company_location == \"CA\"'''\n",
    "\n",
    "# Apply the SQL \"query\"\n",
    "canada_titles = spark.sql(query)\n",
    "\n",
    "# Generate basic statistics\n",
    "canada_titles.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e861f",
   "metadata": {},
   "source": [
    "Aggregating in PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0069f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum salaries for small companies\n",
    "salaries_df.filter(salaries_df.company_size == \"S\").groupBy().min(\"salary_in_usd\").show()\n",
    "\n",
    "# Find the maximum salaries for large companies\n",
    "salaries_df.filter(salaries_df.company_size == \"L\").groupBy().max(\"salary_in_usd\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0608ae1",
   "metadata": {},
   "source": [
    "Aggregating in RDDs ยังงอยู๋เลยงับ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Creation\n",
    "data = [(\"HR\", \"3000\"), (\"IT\", \"4000\"), (\"Finance\", \"3500\")]\n",
    "columns = [\"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Map the DataFrame to an RDD\n",
    "rdd = df.rdd.map(lambda row: (row[\"Department\"], row[\"Salary\"]))\n",
    "\n",
    "# Apply a lambda function to get the sum of the DataFrame\n",
    "rdd_aggregated = rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Show the collected Results\n",
    "print(rdd_aggregated.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7869f",
   "metadata": {},
   "source": [
    "Complex Aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f90be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average salaries at large us companies\n",
    "large_companies=salaries_df.filter(salaries_df.company_size == \"L\").filter(salaries_df.company_location == \"US\").groupBy().avg(\"salary_in_usd\")\n",
    "\n",
    "#set a large companies variable for other analytics\n",
    "large_companies=salaries_df.filter(salaries_df.company_size == \"L\").filter(salaries_df.company_location == \"US\")\n",
    "\n",
    "# Total salaries in usd\n",
    "large_companies.groupBy().sum(\"salary_in_usd\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9baa9d",
   "metadata": {},
   "source": [
    "Bringing it all together I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d131ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "my_spark = SparkSession.builder.appName(\"final_spark\").getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(my_spark)\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "df = my_spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821eb671",
   "metadata": {},
   "source": [
    "Bringing it all together II\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# Perform aggregation\n",
    "agg_result = df.groupBy(\"Department\").sum(\"Salary\")\n",
    "agg_result.show()\n",
    "\n",
    "# Analyze the execution plan\n",
    "agg_result.explain()\n",
    "\n",
    "# Uncache the DataFrame\n",
    "df.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
