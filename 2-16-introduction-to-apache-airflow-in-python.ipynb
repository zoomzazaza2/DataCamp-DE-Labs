{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf56d62",
   "metadata": {},
   "source": [
    "Defining a simple DAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DAG object\n",
    "from airflow import DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7acb0d",
   "metadata": {},
   "source": [
    "Defining a simple DAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8867229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'jdoe',\n",
    "  'start_date': '2023-01-01'\n",
    "}\n",
    "with DAG(\"etl_update\",\n",
    "         default_args=default_args\n",
    "        ):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59db47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "# must import DAG in all file\n",
    "from airflow import DAG\n",
    "\n",
    "default_args = {\n",
    "  'owner': 'jdoe',\n",
    "  'start_date': '2023-01-01'\n",
    "}\n",
    "with DAG(\"etl_update\",\n",
    "         default_args=default_args\n",
    "        ):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22333fe9",
   "metadata": {},
   "source": [
    "Defining a BashOperator task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BashOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "with DAG(dag_id=\"test_dag\", default_args={\"start_date\": \"2024-01-01\"}) as analytics_dag:\n",
    "  # Define the BashOperator \n",
    "  cleanup = BashOperator(\n",
    "      task_id='cleanup_task',\n",
    "      # Define the bash_command\n",
    "      bash_command='cleanup.sh',\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db84b1",
   "metadata": {},
   "source": [
    "Multiple BashOperators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe820a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a second operator to run the `consolidate_data.sh` script\n",
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    bash_command='consolidate_data.sh'\n",
    "    )\n",
    "\n",
    "# Define a final operator to execute the `push_data.sh` script\n",
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push_data.sh'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542065a5",
   "metadata": {},
   "source": [
    "Define order of BashOperators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe5cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new pull_sales task\n",
    "pull_sales = BashOperator(\n",
    "    task_id='pullsales_task',\n",
    "    bash_command='wget https://salestracking/latestinfo?json'\n",
    ")\n",
    "\n",
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup\n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "consolidate << cleanup\n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe1fb3",
   "metadata": {},
   "source": [
    "Using the PythonOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe770b",
   "metadata": {},
   "source": [
    "More PythonOperators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a62869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another Python task\n",
    "parse_file_task = PythonOperator(\n",
    "    task_id='parse_file',\n",
    "    # Set the function to call\n",
    "    python_callable=parse_file,\n",
    "    # Add the arguments\n",
    "    op_kwargs={'inputfile':'latestsales.json', 'outputfile':'parsedfile.json'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32fa4e",
   "metadata": {},
   "source": [
    "EmailOperator and dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Operator\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=process_sales_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> parse_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e2aed4",
   "metadata": {},
   "source": [
    "Schedule a DAG via Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0278b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2023, 11, 1),\n",
    "  # datetime(year,month,day)\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')\n",
    "# cron syntax : 'minutes hours day_of_month month day_of_week'\n",
    "# '* * * * *' : every second?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903fd245",
   "metadata": {},
   "source": [
    "Defining an SLA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2db49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create the dictionary entry\n",
    "default_args = {\n",
    "  'start_date': datetime(2024, 1, 20),\n",
    "  'sla': timedelta(minutes=30)\n",
    "}\n",
    "\n",
    "# Add to the DAG\n",
    "test_dag = DAG('test_workflow', default_args=default_args, schedule_interval=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d1938",
   "metadata": {},
   "source": [
    "Defining a task SLA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5732f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "test_dag = DAG('test_workflow', start_date=datetime(2024,1,20), schedule_interval=None)\n",
    "\n",
    "# Create the task with the SLA\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     sla=timedelta(hours=3),\n",
    "                     bash_command='initialize_data.sh',\n",
    "                     dag=test_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39342fb2",
   "metadata": {},
   "source": [
    "Generate and email a report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the email task\n",
    "email_report = EmailOperator(\n",
    "        task_id='email_report',\n",
    "        to='airflow@datacamp.com',\n",
    "        subject='Airflow Monthly Report',\n",
    "        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n",
    "        files=['monthly_report.pdf'],\n",
    "        dag=report_dag\n",
    ")\n",
    "\n",
    "# Set the email task to run after the report is generated\n",
    "email_report << generate_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf00d3",
   "metadata": {},
   "source": [
    "Adding status emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "default_args={\n",
    "    'email': ['airflowalerts@datacamp.com','airflowadmin@datacamp.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_success': True\n",
    "}\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\",\n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2023,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2023,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec68c75",
   "metadata": {},
   "source": [
    "Creating a templated BashOperator (Jinja)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1090c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Create a templated command to execute\n",
    "# 'bash cleandata.sh datestring'\n",
    "templated_command = \"\"\"\n",
    "    bash cleandata.sh {{ ds_nodash }}\n",
    "\"\"\"\n",
    "# ds คือวันที่ของ current DAG run\n",
    "# {{ ds_nodash }} เป็นตัวแปรพิเศษ ตอน airflow render จะได้ค่า date แบบ nodash Ex. '20251024'\n",
    "# {{ ds }} = date แบบ nodash Ex. '2025-10-24'\n",
    "\n",
    "  \n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          dag=cleandata_dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07550795",
   "metadata": {},
   "source": [
    "Templates with multiple arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the templated command to handle a\n",
    "# second argument called filename.\n",
    "templated_command = \"\"\"\n",
    "  bash cleandata.sh {{ ds_nodash }} {{params.filename}}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to pass the new argument\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filename': 'salesdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "\n",
    "# Create a new BashOperator clean_task2\n",
    "clean_task2 = BashOperator(task_id='cleandata_task2',\n",
    "                           bash_command=templated_command,\n",
    "                           params={'filename': 'supportdata.txt'},\n",
    "                           dag=cleandata_dag)\n",
    "                           \n",
    "# Set the operator dependencies\n",
    "clean_task >> clean_task2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd88250",
   "metadata": {},
   "source": [
    "Using lists with templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "filelist = [f'file{x}.txt' for x in range(30)]\n",
    "\n",
    "print(filelist)\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the template to handle multiple files in a \n",
    "# single run.\n",
    "templated_command = \"\"\"\n",
    "  <% for filename in params.filenames %>\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n",
    "  <% endfor %>\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filenames': filelist},\n",
    "                          dag=cleandata_dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41d9e1",
   "metadata": {},
   "source": [
    "Sending templated emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.email import EmailOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the string representing the html email content\n",
    "html_email_str = \"\"\"\n",
    "Date: {{ ds }}\n",
    "Username: {{ params.username }}\n",
    "\"\"\"\n",
    "\n",
    "email_dag = DAG('template_email_test',\n",
    "                default_args={'start_date': datetime(2023, 4, 15)},\n",
    "                schedule_interval='@weekly')\n",
    "                \n",
    "email_task = EmailOperator(task_id='email_task',\n",
    "                           to='testuser@datacamp.com',\n",
    "                           subject=\"{{ macros.uuid.uuid4() }}\",\n",
    "                          # {{ macros.uuid.uuid4() }} → สร้าง UUID แบบ random\n",
    "                          # {{ macros.uuid.uuid1() }} → UUID จากเวลา + host ID\n",
    "                           html_content=html_email_str,\n",
    "                           params={'username': 'testemailuser'},\n",
    "                           dag=email_dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72bbba",
   "metadata": {},
   "source": [
    "Define a BranchPythonOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7378243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to determine if years are different\n",
    "def year_check(**kwargs):\n",
    "    current_year = int(kwargs['ds'][0:4])\n",
    "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
    "    if current_year == previous_year:\n",
    "        return 'current_year_task'\n",
    "    else:\n",
    "        return 'new_year_task'\n",
    "    \n",
    "# prev_ds คือวันที่ของ last DAG run\n",
    "\n",
    "# Define the BranchPythonOperator\n",
    "branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n",
    "                                   python_callable=year_check, provide_context=True)\n",
    "# Define the dependencies\n",
    "branch_task >> current_year_task\n",
    "branch_task >> new_year_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d938526",
   "metadata": {},
   "source": [
    "Creating a production pipeline #1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "# Import the needed operators\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import date, datetime\n",
    "\n",
    "def process_data(**context):\n",
    "  file = open('/home/repl/workspace/processed_data.tmp', 'w')\n",
    "  file.write(f'Data processed on {date.today()}')\n",
    "  file.close()\n",
    "\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2023,4,1)})\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    # use touch /home/repl/workspace/startprocess.txt to address No file found error\n",
    "                    poke_interval=5,\n",
    "                    timeout=15,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b5b6a",
   "metadata": {},
   "source": [
    "Creating a production pipeline #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from dags.process import process_data\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Update the default arguments and apply them to the DAG\n",
    "default_args = {\n",
    "  'start_date': datetime(2023,1,1),\n",
    "  'sla':timedelta(minutes=90)\n",
    "}\n",
    "\n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca6ff4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a72dc49",
   "metadata": {},
   "source": [
    "Adding the final changes to your pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731efa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Update the default arguments and apply them to the DAG.\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2023,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "email_subject=\"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "email_report_task = EmailOperator(task_id='email_report_task',\n",
    "                                  to='sales@mycompany.com',\n",
    "                                  subject=email_subject,\n",
    "                                  html_content='',\n",
    "                                  params={'department': 'Data subscription services'},\n",
    "                                  dag=dag)\n",
    "\n",
    "no_email_task = EmptyOperator(task_id='no_email_task', dag=dag)\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return 'email_report_task'\n",
    "    else:\n",
    "        return 'no_email_task'\n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
    "                                   python_callable=check_weekend,\n",
    "                                   provide_context=True,                                \n",
    "                                   dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
